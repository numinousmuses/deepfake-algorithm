{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/joshuaokolo/deepfake-algorithm?scriptVersionId=104053877\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Deepfake Algorithm\n\nAlgorithm for detecting deepfakes","metadata":{}},{"cell_type":"markdown","source":"## Tenforflow Input Pipeline","metadata":{}},{"cell_type":"code","source":"# Parse the Image\n\ndef parse_function(filename, label):\n\n    IMG_SHAPE = 224\n\n    image_string = tf.io.read_file(filename)\n\n    image = tf.image.decode_png(image_string, channels=3)\n\n    #This will convert to float values in [0, 1]\n\n    image = tf.image.convert_image_dtype(image, tf.float32)\n\n    resized_image = tf.image.resize(image, [IMG_SHAPE, IMG_SHAPE])\n\n    return resized_image, label","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Frequency Domain Model","metadata":{}},{"cell_type":"code","source":"# Applying Discrete Cosine Transformation - II on the images\n\ndef dct_preprocess(image, label):\n\n    img_t = tf.transpose(image,perm=[2, 1, 0])\n\n    X1 = tf.signal.dct(img_t, type=2, norm=\"ortho\")\n\n    X1_t = tf.transpose(X1,perm=[0, 2, 1])\n\n    X2 = tf.signal.dct(X1_t, type=2, norm=\"ortho\")   \n\n    array_X2 = tf.transpose(X2, perm=[1, 2, 0])\n\n    # converting dct coefficients into log scale\n\n    epsilon=1e-12\n    \n    array_X2_log = tf.math.log(tf.math.abs(array_X2) + epsilon)\n\n    return array_X2_log, label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input pipeline for training and validation dataset\n\ntrain_ratio   = 0.80\n\ntrain_dataset = all_train_dataset.take(ds_size*train_ratio)\n\nvalid_dataset = all_train_dataset.skip(ds_size*train_ratio)\n\nbatch_size    = 40\n\ntrain_dataset = train_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE,deterministic=False)\n\ntrain_dataset = train_dataset.map(dct_preprocess, num_parallel_calls=tf.data.AUTOTUNE,deterministic=False)\n\ntrain_dataset = train_dataset.batch(batch_size)\n\ntrain_dataset = train_dataset.prefetch(tf.data.AUTOTUNE).cache()\n\nvalid_dataset = valid_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE,deterministic=False)\n\nvalid_dataset = valid_dataset.map(dct_preprocess, num_parallel_calls=tf.data.AUTOTUNE,deterministic=False)\n\nvalid_dataset = valid_dataset.batch(batch_size)\n\nvalid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE).cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simple CNN Model ( using DCT-II pre-processing)\n\nIMG_SHAPE = 224\n\nx   = Input(shape = (IMG_SHAPE, IMG_SHAPE, 3))\n\nx1  = Conv2D(3, 3, padding=\"same\", activation=\"relu\")(x)\n\nx1  = BatchNormalization()(x1)\n\nx2  = Conv2D(8, 3, padding=\"same\", activation=\"relu\")(x1)\n\nx2  = BatchNormalization()(x2)\n\nx2  = AveragePooling2D()(x2)  # 64\n\nx3  = Conv2D(16, 3, padding=\"same\", activation=\"relu\")(x2)\n\nx3  = BatchNormalization()(x3)\n\nx3  = AveragePooling2D()(x3)  # 32\n\nx4  = Conv2D(32, 3, padding=\"same\", activation=\"relu\")(x3)\n\nx4  = BatchNormalization()(x4)\n\ny   = Flatten()(x4)\n\ny   = Dropout(0.5)(y)\n\ny   = Dense(1, activation='sigmoid')(y)\n\nmodel = KerasModel(inputs=x, outputs=y)\n\nmodel.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spatial Domain Model","metadata":{}},{"cell_type":"code","source":"# Image Augmentation\n\ndef train_preprocess(image, label):\n\n    IMG_SHAPE = 224\n\n    image = tf.image.random_flip_left_right(image)\n\n    image = tf.image.random_flip_up_down(image)\n\n    image = tf.image.random_brightness(image, max_delta=32.0 /  \n\n           255.0)\n\n    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n\n  # random gaussian filter\n\n    if tf.random.uniform(shape=[], minval=0.0, maxval=1.0) < 0.5:\n\n        image = tfa.image.gaussian_filter2d(image)\n\n    else:\n\n        image\n\n   # random invert image\n\n    if  tf.random.uniform([]) < 0.5:\n\n        image = (1-image)\n\n    else:\n\n        image\n\n   # random crop\n\n    if  tf.random.uniform([]) < 0.5:\n\n        image = tf.image.resize(tf.image.central_crop(image,\n\n        central_fraction=0.5),[IMG_SHAPE, IMG_SHAPE])\n\n    else:\n\n        image\n\n    # random rotate\n\n    if  tf.random.uniform([]) < 0.5:\n\n        image = tf.image.rot90(image)\n\n    else:\n\n        image\n\n        image = tf.clip_by_value(image, 0.0, 1.0)\n\n    return image, label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training and validation dataset\n\ntrain_ratio   = 0.80\n\ntrain_dataset = all_train_dataset.take(ds_size*train_ratio)\n\nvalid_dataset = all_train_dataset.skip(ds_size*train_ratio)\n\nbatch_size    = 40\n\ntrain_dataset = train_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE,deterministic=False)\n\ntrain_dataset = train_dataset.map(train_preprocess, num_parallel_calls=tf.data.AUTOTUNE,deterministic=False)\n\ntrain_dataset = train_dataset.batch(batch_size)\n\ntrain_dataset = train_dataset.prefetch(tf.data.AUTOTUNE).cache()\n\nvalid_dataset = valid_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE,deterministic=False)\n\nvalid_dataset = valid_dataset.batch(batch_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spatial Domain Model ( Meso-inceptionnet)\n\n# define inception layer\n\ndef InceptionLayer(a, b, c, d):\n\n    def func(x):\n\n    x1 = Conv2D(a, (1, 1), padding='same', activation='relu')(x)\n\n    x2 = Conv2D(b, (1, 1), padding='same', activation='relu')(x)\n\n    x2 = Conv2D(b, (3, 3), padding='same', activation='relu')(x2)\n\n    x3 = Conv2D(c, (1, 1), padding='same', activation='relu')(x)\n\n    x3 = Conv2D(c, (3, 3), dilation_rate = 2, strides = 1,    \n\n         padding='same', activation='relu')(x3)\n\n    x4 = Conv2D(d, (1, 1), padding='same', activation='relu')(x)\n\n    x4 = Conv2D(d, (3, 3), dilation_rate = 3, strides = 1, \n\n         padding='same', activation='relu')(x4)\n\n    y = Concatenate(axis = -1)([x1, x2, x3, x4])\n\n    return y\n\n return func\n\n# meso - inception net model\n\nIMG_SHAPE = 224\n\nx = Input(shape = (IMG_SHAPE, IMG_SHAPE, 3))\n\nx1 = InceptionLayer(1, 4, 4, 2)(x)\n\nx1 = BatchNormalization()(x1)\n\nx1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n\nx2 = InceptionLayer(2, 4, 4, 2)(x1)\n\nx2 = BatchNormalization()(x2)\n\nx2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)\n\nx3 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x2)\n\nx3 = BatchNormalization()(x3)\n\nx3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n\nx4 = Conv2D(16, (5, 5), padding='same', activation = 'relu')(x3)\n\nx4 = BatchNormalization()(x4)\n\nx4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n\ny = Flatten()(x4)\n\ny = Dropout(0.5)(y)\n\ny = Dense(16)(y)\n\ny = LeakyReLU(alpha=0.1)(y)\n\ny = Dropout(0.5)(y)\n\ny = Dense(1, activation = 'sigmoid')(y)\n\nmodel = KerasModel(inputs = x, outputs = y)\n\nmodel.compile(loss='mean_squared_error',optimizer=\"adam\",metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combining both models: Cross Domain Model","metadata":{}},{"cell_type":"code","source":"@tf.function\n\ndef train_preprocess(image, label):\n\n    IMG_SHAPE = 224\n\n    image1 = tf.image.random_flip_left_right(image)\n\n    image1 = tf.image.random_flip_up_down(image)\n\n    image1 = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n\n    image1 = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n\n    # random gaussian filter\n\n    if tf.random.uniform(shape=[], minval=0.0, maxval=1.0) < 0.5:\n\n        image1 = tfa.image.gaussian_filter2d(image)\n\n    else:\n\n        image1 = image\n\n    # random invert image\n\n    if  tf.random.uniform([]) < 0.5:\n\n        image1 = (1-image)\n\n    else:\n\n        image1 = image\n\n    # random crop\n\n    if  tf.random.uniform([]) < 0.5:\n\n        image1 = tf.image.resize(tf.image.central_crop(image, central_fraction=0.5),[IMG_SHAPE, IMG_SHAPE])\n\n    else:\n\n        image1 = image\n\n    # random rotate\n\n    if  tf.random.uniform([]) < 0.5:\n\n        image1 = tf.image.rot90(image)\n\n    else:\n\n        image1 = image\n\n        image1 = tf.clip_by_value(image, 0.0, 1.0)\n\n    return image1,image, label\n\n@tf.function\n\ndef dct_preprocess(image1,image, label):\n\n    img_t = tf.transpose(image,perm=[2, 1, 0])\n\n    X1 = tf.signal.dct(img_t, type=2, norm=\"ortho\")\n\n    X1_t = tf.transpose(X1,perm=[0, 2, 1])\n\n    X2 = tf.signal.dct(X1_t, type=2, norm=\"ortho\")\n\n    array_X2 = tf.transpose(X2, perm=[1, 2, 0])\n\n    epsilon=1e-12\n\n    array_X2_log = tf.math.log(tf.math.abs(array_X2) + epsilon)\n\n    return (image1,array_X2_log), label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training and validation dataset\n\ntrain_ratio   = 0.80\n\ntrain_dataset = all_train_dataset.take(ds_size*train_ratio)\n\nvalid_dataset = all_train_dataset.skip(ds_size*train_ratio)\n\nbatch_size    = 40\n\ntrain_dataset = train_dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE,deterministic=False)\n\ntrain_dataset = train_dataset.map(train_preprocess, num_parallel_calls=tf.data.AUTOTUNE,deterministic=False)\n\ntrain_dataset = train_dataset.map(dct_preprocess, num_parallel_calls=tf.data.AUTOTUNE,deterministic=False)\n\ntrain_dataset = train_dataset.batch(batch_size)\n\ntrain_dataset = train_dataset.prefetch(tf.data.AUTOTUNE).cache()\n\nThe architecture of the cross domain model concatenates the output of spatial domain model (y1) and frequency domain model (y2) into single input y as shown in the code block below. It is then followed by a dense layer and output layer. \n\n## add the model layers\n\ny = tf.keras.layers.Concatenate()([y1,y2])\n\ny = Dropout(0.5)(y)\n\ny = Dense(64)(y)\n\ny = LeakyReLU(alpha=0.1)(y)\n\ny = Dropout(0.5)(y)\n\ny = Dense(1, activation = 'sigmoid')(y)\n\nmodel = KerasModel(inputs = [x,u], outputs = y)\n\n#early stopping to monitor the validation loss and avoid overfitting\n\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n\n#reducing learning rate on plateau\n\nrlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience= 5, factor= 0.5, min_lr= 1e-6, verbose=1)\n\nmodel.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References\n\nhttps://arxiv.org/abs/1809.00888\n\nhttps://en.wikipedia.org/wiki/Discrete_cosine_transform\n\nhttps://en.wikipedia.org/wiki/Discrete_cosine_transform","metadata":{}}]}